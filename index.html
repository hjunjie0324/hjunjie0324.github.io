<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"hjunjie0324.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="韩君杰的个人博客">
<meta property="og:url" content="https://hjunjie0324.github.io/index.html">
<meta property="og:site_name" content="韩君杰的个人博客">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Junjie Han">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://hjunjie0324.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>韩君杰的个人博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta custom-logo">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">韩君杰的个人博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">blog of a NLPer</p>
      <a>
        <img class="custom-logo-image" src="/images/profile_photo.jpeg" alt="韩君杰的个人博客">
      </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hjunjie0324.github.io/2022/02/27/%E5%9F%BA%E4%BA%8E%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8CHMM%E7%9A%84%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Junjie Han">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="韩君杰的个人博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/27/%E5%9F%BA%E4%BA%8E%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8CHMM%E7%9A%84%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/" class="post-title-link" itemprop="url">基于语言模型和HMM的词性标注</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-02-26 16:20:49 / Modified: 08:25:14" itemprop="dateCreated datePublished" datetime="2022-02-26T16:20:49Z">2022-02-26</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>基于bigram语言模型<br>基于trigram语言模型</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hjunjie0324.github.io/2021/01/10/%E5%9F%BA%E4%BA%8ESQuAD%E7%9A%84%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%BA%8C%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Junjie Han">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="韩君杰的个人博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/10/%E5%9F%BA%E4%BA%8ESQuAD%E7%9A%84%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%BA%8C%EF%BC%89/" class="post-title-link" itemprop="url">基于SQuAD的问答系统（二）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-09 18:20:39" itemprop="dateCreated datePublished" datetime="2021-01-09T18:20:39Z">2021-01-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-01-10 05:22:02" itemprop="dateModified" datetime="2021-01-10T05:22:02Z">2021-01-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>上一篇我们写了使用bert fine tune的方法搭建问答系统。这一篇我们使用bert+BiDAF的方法，即在bert之上再加一个BiDAF模块。<br>完整代码见<a target="_blank" rel="noopener" href="https://github.com/hlnchen/BERT_with_Att_Flow">github</a></p>
<h2 id="什么是BiDAF"><a href="#什么是BiDAF" class="headerlink" title="什么是BiDAF"></a>什么是BiDAF</h2><p>原始论文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.01603">BiDAF</a><br>BiDAF是阅读理解以及问答系统领域比较重要的模型。在BiDAF之前，一个stanford attentive reader的模型比较流行，其核心就是context里的每个词和question做简单的attention。BiDAF则改进了attention部分。BiDAF paper指出之前的attention方法犯了”early summarization”的问题，使得计算attention过程中信息损失过多。所以在BiDAF模型中我们分步计算attention，对文本慢慢的进行summary,减少了信息损失。另外这个attention的计算方法是memory-less的，也就是说每一步计算只需要前一步的结果。</p>
<h2 id="preprocess"><a href="#preprocess" class="headerlink" title="preprocess"></a>preprocess</h2><p>preprocess的大致过程跟之前的bert fine tune差不多，也是通过tokenizer得到一个encoding。但是在BiDAF模型中需要把question和context分开来。所以在encoding中每条数据的question长度需要一样，每条数据的context长度需要一样。这里我们设置question最长为62，手动对encodings中的question和context部分做padding和truncation。</p>
<h2 id="model"><a href="#model" class="headerlink" title="model"></a>model</h2><p>原始的BiDAF论文使用了character level embedding（字母向量），但是在我们的项目中character level embedding跟bert耦合起来比较麻烦。再加之character level embedding对模型的提升不是很大，所以我们这里没有使用character level embedding。<br>下面这张图是我们模型的架构。<br><img src="/images/bidaf.png" alt="avatar"></p>
<p>第一层当然是bert。我们用bert输出的word embedding作为question feature和context feature   </p>
<p>接下来是attention layer<br>attention layer是BiDAF模型中最重要的一层。分步计算attention避免了early summarization的问题，减少了信息的损失。之前模型的attention一般只计算question to context的attention, 而在BiDAF模型中我们要计算question to context和context to question两个方向的attention。另外这一层的attention也不是一个简单的点乘，而是包含了需要训练的参数。具体步骤请参考BiDAF的原始论文。最后我们把获得的question-aware conetxt representation和之前bert输出的word embedding拼接起来输入给下一层。</p>
<p>再然后是Bidirectional LSTM layer<br>这一层主要用来学习有了question attention之后，词与词之间的关系。加上Bi-LSTM layer之后模型运行的速度显著的变慢，一个GPU运行4个epoch的时间差不多要40个小时。可以考虑用transformer代替Bi-LSTM</p>
<p>最后是prediction layer<br>这一层我们用linear layer来输出每个token的start_logit和end_logit，作为预测token是否为起始位置或者结束位置的概率。最后我们用nn.CrossEntropyLoss()来计算loss</p>
<h2 id="train"><a href="#train" class="headerlink" title="train"></a>train</h2><p>因为机器内存的限制我们设置batch size为6。另外epoch次数为6，优化器用adam,学习率为3e-5。</p>
<h2 id="result-and-analysis"><a href="#result-and-analysis" class="headerlink" title="result and analysis"></a>result and analysis</h2><p>在evaluation dataset的测试显示F1 score为0.72。<br>使用attention除了可以提升模型性能之外，还有一个好处就是比较容易可视化，很方便的就可以看出哪些词之间attention比较高。比如下面这张attention map<br><img src="/images/att.jpg" alt="avatar"><br>这条数据我们模型作出了正确的回答，答案是Norway。从图中可以看的Norway是attention比较大的词。</p>
<h2 id="follow-up"><a href="#follow-up" class="headerlink" title="follow up"></a>follow up</h2><p>总的来说结果并不算特别好。可能是因为BiDAF模型本身不够强大。想要提高分数我们可以尝试其他的模型，比如SA-net。我们也可以尝试ensemble model。</p>
<p>另一方面可以从学习策略方面进行改进。比如尝试使用curriculum learning或者anti-curriculum learning。</p>
<p><img src="/images/like.jpeg" alt="avatar"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hjunjie0324.github.io/2021/01/09/%E5%9F%BA%E4%BA%8ESQuAD%E7%9A%84%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%B8%80%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Junjie Han">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="韩君杰的个人博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/09/%E5%9F%BA%E4%BA%8ESQuAD%E7%9A%84%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%B8%80%EF%BC%89/" class="post-title-link" itemprop="url">基于SQuAD的问答系统（一）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-09 13:10:54" itemprop="dateCreated datePublished" datetime="2021-01-09T13:10:54Z">2021-01-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-01-10 00:59:22" itemprop="dateModified" datetime="2021-01-10T00:59:22Z">2021-01-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>问答系统是自然语言处理一个重要的下游任务。这里我们基于SQuAD数据集构建问答系统<a target="_blank" rel="noopener" href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD</a></p>
<p>SQuAD数据集由斯坦福发布。数据集中所有问题都是factoid question，即问题可以由长文本中的一个词或者一个短语回答。每条数据会给出一个问题和一个长文本，问答系统负责把可以作为答案的短语从长文本中找出来。所以基于SQuAD的问答系统不用考虑判断对错，计数等问题，只需要考虑factoid question，可以作为构建一个可以在现实世界中使用的问答系统非常好的起点。</p>
<p>SQuAD最新的2.0版本相对于之前的版本加入了no answer的情况，也就是给出的长文本中没有问题的答案，这样使得问答系统必须更加真正理解问题和长文本。</p>
<p>这篇文章我们介绍bert fine tune的方法。下一篇文章我们将会介绍bert + BiDAF的方法。<br>这篇文章的完整代码<a target="_blank" rel="noopener" href="https://github.com/hjunjie0324/SQuAD_bert_fine_tune">github</a></p>
<h2 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h2><p>因为是factoid question，答案在长文本中，所以模型只需要输出答案短语在长文本中的起始位置和结束位置。因此在训练中我们把起始位置和结束位置作为label，模型的任务就是挑出最有可能作为起始位置的词以及最有可能作为结束位置的词。</p>
<h2 id="explore-data"><a href="#explore-data" class="headerlink" title="explore data"></a>explore data</h2><p>bert每次可以处理的最长文本为512，再长就需要truncation或者将文本分批次输入给bert。<br>训练集中的长文本长度绝大部分在400以下，极少数超过了512，需要truncation</p>
<h2 id="preprocess"><a href="#preprocess" class="headerlink" title="preprocess"></a>preprocess</h2><p>训练集只提供了答案的character start position，我们需要添加end position,并且把它们转化为token position。如果是no-answer的情况，根据bert paper,我们把start position和end position都放在起始的CLS token上。<br>之后我们用tokenizer将文本转化为可以输入bert模型的encoding。这里我们使用huggingFace的BertTokenizerFast。把question和context一起输入tokenizer得到encodings</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = BertTokenizerFast.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line">train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)</span><br></pre></td></tr></table></figure>
<p>输出的encoding是字典的结构，包含了input_ids和attention_mask。input_ids是每个token在bert中的id，问题和文本之间用SEP token分开。attention_mask表示该位置的token是否是padding。</p>
<h2 id="handle-no-answer-question"><a href="#handle-no-answer-question" class="headerlink" title="handle no-answer question:"></a>handle no-answer question:</h2><p>遇到no-answer的情况，我们把start_position和end_position都设置在起始的CLS token上。预测是否为no-answer question的时候，我们把CLS token为start position和end_position的概率加起来，同时把另外非CLS的token中，start_position和end_position概率最大的加起来，将两者进行比较。可以在前者加上一个threshold。</p>
<h2 id="model-and-train"><a href="#model-and-train" class="headerlink" title="model and train"></a>model and train</h2><p>模型我们直接采用了huggingFace的BertForQuestionAnswering。也就是bert输出的hidden state上面加了一层linear layer用来计算每个token是start position或者是end position的概率。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = BertForQuestionAnswering.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>,return_dict=True)</span><br></pre></td></tr></table></figure>
<p>引入SquadDataset类，继承Dataset类,将之前的train_encoding转化为SquadDataset。Dataset是pytorch提供的数据读取接口。__getitem__方法使得每一条数据都可以通过dataset[i]的方法读取</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">class SquadDataset(torch.utils.data.Dataset):</span><br><span class="line"> def __init__(self,encodings):</span><br><span class="line">   self.encodings = encodings</span><br><span class="line"> def __getitem__(self,idx):</span><br><span class="line">   <span class="built_in">return</span> &#123;key:torch.tensor(val[idx]) <span class="keyword">for</span> key, val <span class="keyword">in</span> self.encodings.items()&#125;</span><br><span class="line"> def __len__(self):</span><br><span class="line">   <span class="built_in">return</span> len(self.encodings.input_ids)</span><br><span class="line"></span><br><span class="line">train_dataset = SquadDataset(train_encodings)</span><br></pre></td></tr></table></figure>
<p>把dataset放入dataLoader里。dataLoader主要是用来产生一个batch的数据</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_loader = DataLoader(small_train_dataset,batch_size=16,shuffle=True)</span><br></pre></td></tr></table></figure>
<p>bert paper推荐设定batch size为48进行训练，但是我们没有那么大的内存空间，所以我们设置batch size为16，epoch为3，adam优化器的学习率为5e-5</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(3):</span><br><span class="line"> <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">   optim.zero_grad()</span><br><span class="line">   input_ids = batch[<span class="string">&#x27;input_ids&#x27;</span>].to(device)</span><br><span class="line">   attention_mask = batch[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">   start_positions = batch[<span class="string">&#x27;start_positions&#x27;</span>].to(device)</span><br><span class="line">   end_positions = batch[<span class="string">&#x27;end_positions&#x27;</span>].to(device)</span><br><span class="line">   outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions,</span><br><span class="line">                   end_positions=end_positions)</span><br><span class="line">   loss = outputs[0]</span><br><span class="line">   loss.backward()</span><br><span class="line">   optim.step()</span><br></pre></td></tr></table></figure>
<h2 id="result"><a href="#result" class="headerlink" title="result"></a>result</h2><p>在evaluation dataset上的测试结果显示: accuracy为0.61, f1 score为0.67。</p>
<p>如果觉得这篇文章有帮助的话，扫码点个赞呗<br><img src="/images/like.jpeg" alt="avatar"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hjunjie0324.github.io/2021/01/09/real-or-not-tweets-classification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Junjie Han">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="韩君杰的个人博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/09/real-or-not-tweets-classification/" class="post-title-link" itemprop="url">real or not: tweets classification</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-08 17:46:39" itemprop="dateCreated datePublished" datetime="2021-01-08T17:46:39Z">2021-01-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-01-09 04:29:42" itemprop="dateModified" datetime="2021-01-09T04:29:42Z">2021-01-09</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>这是kaggle的一个关于文本分类的比赛: Natural Language Processing with Disaster Tweets. <a target="_blank" rel="noopener" href="https://www.kaggle.com/c/nlp-getting-started">kaggle</a> 任务就是判断网友所发的推特是不是真的灾难。举个例子，一个叫Anna的网友发推: “On plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE”. ABLAZE有烈火熊熊燃烧的例子。在这里ABLAZE是形容火烧云的，但对于机器可能会误判为形容山火之类的灾难。</p>
<p>题目看完了，有什么想法嘛？我的首先这是一个二分类的问题。比较传统的方法可以用logistic regression之类的。也可以用bert的CLS token来fine tune。</p>
<p>排行榜上这个比赛是用F1 score进行排名。最后我的成绩是0.837,大概排在150位左右。顺便一提，排行榜上前60位的F1 score都是1，非常奇怪。讨论区有人说这是因为测试集的标签是公开的，这些人就直接上传了正确的标签进行作弊。而且他们没有人上传代码，所以我认为他们可能都是作弊的吧。</p>
<p>本文bert fine tune部分的代码见<a target="_blank" rel="noopener" href="https://github.com/hjunjie0324/real_or_not">github</a></p>
<h2 id="explore-data"><a href="#explore-data" class="headerlink" title="explore data"></a>explore data</h2><p>先看一下训练集的类别是否平衡。如果训练集中某一个类别的数据非常少我们可能需要data augmentation，人为提高这个类别的数据量。这里训练集中 not_real的数据有4000多条, real的数据有3000多条,类别还是比较平衡的。<br><img src="/images/label_balance.png" alt="avatar"></p>
<p>再看一下每条文本的长短。这个项目中文本算是比较短的 (因为大家发推特都不会太长)。最长的文本也只有30多个词。<br><img src="/images/num_words.png" alt="avatar"></p>
<h2 id="baseline-countVector-ridgeRegression"><a href="#baseline-countVector-ridgeRegression" class="headerlink" title="baseline: countVector + ridgeRegression"></a>baseline: countVector + ridgeRegression</h2><p>首先试一下kaggle官方提供的baseline model.模型简单到飞起，甚至没有用词向量，而是直接用了countVectorizer</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">count_vectorizer = feature_extraction.text.CountVectorizer()</span><br><span class="line">train_vectors = count_vectorizer.fit_transform(train_df[<span class="string">&quot;text&quot;</span>])</span><br><span class="line">test_vectors = count_vectorizer.transform(test_df[<span class="string">&quot;text&quot;</span>])</span><br></pre></td></tr></table></figure>
<p>由此我们把整个文本数据转化成了大矩阵，这个矩阵表示每条text当中每个单词出现的次数，而且这个矩阵比较稀疏。</p>
<p>接下来我们调用sklearn包中的ridge regression model进行训练和预测</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">clf = linear_model.RidgeClassifier()</span><br><span class="line">clf.fit(train_vectors,train_df[<span class="string">&#x27;target&#x27;</span>])</span><br><span class="line">prediction = clf.predict(test_vectors)</span><br></pre></td></tr></table></figure>
<p>ridge regession是linear regression的变体，就是在标准linear regression的基础上加上了L2 regularization，用来解决过拟合等问题。<br>这个baseline模型非常简单，但效果却出乎意料的好，在测试集上的F1 score达到了0.78. 所以对于这个数据集来说，word count可以很好的表示推文是否是真正的灾难。这确实挺出乎我的意料的。因为word count只能简单的表示句子中各个单词出现的次数，很难去捕捉到句子真正的含义。我想了想，可能是因为训练集相对比较小的缘故(7000多条),使得一些比较复杂的模型很难通过训练集学到合适的参数，所以像word count这类word representation可能会比较有优势。同时，所用到的句子都比较短，最长的也只有50几个token，这可呢能使得模型通过一些词就可以学习到句子的含义。</p>
<h2 id="bert-fine-tune"><a href="#bert-fine-tune" class="headerlink" title="bert fine tune"></a>bert fine tune</h2><p>根据bert paper所写，final hidden state of CLS token is used as the aggregate sequence representation for classification tasks. 所以在这里我们就只取CLS token的final hidden state来作为整个句子的表示进行分类任务。</p>
<h4 id="preprocess"><a href="#preprocess" class="headerlink" title="preprocess"></a>preprocess</h4><p>我们先清洗一下数据。像这种社交媒体上的数据一般来说会比较杂乱，不像官方新闻那样很有条理，所以数据清洗的任务也相对繁琐一点。<br>首先我们去掉url</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def remove_URL(text):</span><br><span class="line">   url = re.compile(r<span class="string">&#x27;https?://\S+|www\.\S+&#x27;</span>)</span><br><span class="line">   <span class="built_in">return</span> url.sub(r<span class="string">&#x27;&#x27;</span>,text)</span><br></pre></td></tr></table></figure>
<p>然后去掉表情包</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def remove_emoji(text):</span><br><span class="line">   emoji_pattern = re.compile(<span class="string">&quot;[&quot;</span></span><br><span class="line">                          u<span class="string">&quot;\U0001F600-\U0001F64F&quot;</span>  <span class="comment"># emoticons</span></span><br><span class="line">                          u<span class="string">&quot;\U0001F300-\U0001F5FF&quot;</span>  <span class="comment"># symbols &amp; pictographs</span></span><br><span class="line">                          u<span class="string">&quot;\U0001F680-\U0001F6FF&quot;</span>  <span class="comment"># transport &amp; map symbols</span></span><br><span class="line">                          u<span class="string">&quot;\U0001F1E0-\U0001F1FF&quot;</span>  <span class="comment"># flags (iOS)</span></span><br><span class="line">                          u<span class="string">&quot;\U00002702-\U000027B0&quot;</span></span><br><span class="line">                          u<span class="string">&quot;\U000024C2-\U0001F251&quot;</span></span><br><span class="line">                          <span class="string">&quot;]+&quot;</span>, flags=re.UNICODE)</span><br><span class="line">   <span class="built_in">return</span> emoji_pattern.sub(r<span class="string">&#x27;&#x27;</span>,text)</span><br></pre></td></tr></table></figure>
<p>去掉html</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def remove_html(text):</span><br><span class="line">   html = re.compile(r<span class="string">&#x27;&lt;.*?&gt;&#x27;</span>)</span><br><span class="line">   <span class="built_in">return</span> html.sub(r<span class="string">&#x27;&#x27;</span>,text)</span><br></pre></td></tr></table></figure>
<p>数据清洗的工作就差不多完成啦<br>接下来是tokenization<br>这里我们调用huggingFace的tokenizer</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line">train_encodings = tokenizer(text_train, truncation = True, padding = True)</span><br></pre></td></tr></table></figure>
<p>这里tokenizer把text分割成一个一个token,并且生成input_ids和attention_mask。其中input_ids是各个token的id，attention_mask表示所在位置的input_ids是不是padding。</p>
<p>这里有一个小问题，tokenizer这里是用输入的最长的text来作为max length进行encode。训练集当中，最长的text长度为55，测试集中最长的text长度为54。这两者不一致。我这里设定max length为60并手动把input_ids 和 attention_mask 加padding到60。大家也可以调用tokenizer.encode_plus()方法来设定max length或者把训练集和测试集拼接到一起进行encode。</p>
<h4 id="model"><a href="#model" class="headerlink" title="model"></a>model</h4><p>接下来就是模型啦。<br>第一层当然是bert</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.bert = BertModel.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>取出CLS token的last hidden state作为提取的feature</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)</span><br><span class="line">last_hidden_state_cls = bert_output[0][:,0,:]</span><br></pre></td></tr></table></figure>
<p>last_hidden_state_cls的shape是batch_size * 768。 这里768是hidden vector的维度<br>因为这是二分类问题，所以我们要把维度从768降到2</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">D_in = 768</span><br><span class="line">D_hidden = 50</span><br><span class="line">D_out = 2</span><br><span class="line">self.bert = BertModel.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line">self.classifier = nn.Sequential(</span><br><span class="line">    nn.Linear(D_in, D_hidden),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Dropout(0.1),</span><br><span class="line">    nn.Linear(D_hidden, D_out)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这里dropout layer是指按10%的概率随机抛弃掉上一层神经网络的特征。这样使得神经网络不会过于依赖某几个特征，也降低了过拟合的问题。</p>
<p>计算loss的时候我们使用分类问题常用的cross entropy</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss_func = nn.CrossEntropyLoss()</span><br><span class="line">loss = loss_func(output, target)</span><br></pre></td></tr></table></figure>
<p>这里nn.CrossEntropyLoss结合了nn.LogSoftmax()和nn.NLLLoss()</p>
<h4 id="hyperparameter"><a href="#hyperparameter" class="headerlink" title="hyperparameter"></a>hyperparameter</h4><p>优化器使用adam，学习率为3e-5。batch size为16, epoch次数为4。<br>这里对优化器加上了linear scheuler</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps = total_steps)</span><br></pre></td></tr></table></figure>
<p>这样学习率为随着迭代线性降低。</p>
<p>为了防止梯度爆炸的问题，加上了clip gradient</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.utils.clip_grad_norm_(model.parameters(),1.0)</span><br></pre></td></tr></table></figure>
<h4 id="prediction-layer"><a href="#prediction-layer" class="headerlink" title="prediction layer"></a>prediction layer</h4><p>对测试集进行预测的时候加上了softmax函数将其转化为概率。根据之前模型在val_dataset上的表现将threshold定为0.9。最后测试集的F1 score为0.837。</p>
<h2 id="follow-up"><a href="#follow-up" class="headerlink" title="follow up"></a>follow up</h2><p>总的来说这次结果还算ok，但还是留下了改进的空间。这里bert fine tune是只用了CLS token来表示整个句子。根据一些paper, 只用CLS token来表示句子可能是比较弱的。之后我们可以试着用整个句子的word embedding来表示，可能会取得更好的效果。<br>另外，bert模型是在bookCorpus和英语维基百科上进行预训练的，是比较正规的英语。这个项目中用到的是推特的数据集，这种社交网站上的数据相对来说比较杂乱和随意的，跟bert预训练所用的数据集可以看作是不同的domain，词汇的分布等等都不是很一致，所以可以在bert预训练的基础上，再在推特数据集上再次进行预训练。</p>
<p>如果觉得这篇文章有帮助的话，扫码点个赞呗<br><img src="/images/like.jpeg" alt="avatar"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hjunjie0324.github.io/2020/12/18/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Junjie Han">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="韩君杰的个人博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/12/18/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-12-18 09:23:33" itemprop="dateCreated datePublished" datetime="2020-12-18T09:23:33Z">2020-12-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-01-10 06:02:10" itemprop="dateModified" datetime="2021-01-10T06:02:10Z">2021-01-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>大家好，欢迎来到我的博客。我是韩君杰。硕士毕业于加州大学戴维斯分校。专业是大气科学，但是却跑去搞计算机。目前主攻nlp方向。底下是我的微信，欢迎大家交流以及打赏。</p>
<p><img src="/images/wechat_channel.jpeg" alt="avatar"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Junjie Han</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/hjunjie0324" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hjunjie0324" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.instagram.com/hanjunjie0324/" title="Instagram → https:&#x2F;&#x2F;www.instagram.com&#x2F;hanjunjie0324&#x2F;" rel="noopener" target="_blank"><i class="fab fa-instagram fa-fw"></i>Instagram</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Junjie Han</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
